{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytics - HW 2 \n",
    "## Finn Qiao\n",
    "\n",
    "## 1. Implementation of different CBOW and Skip-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/finn/Downloads/amazon-fine-food-reviews/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId ProfileName  HelpfulnessNumerator  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW  delmartian                     1   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK      dll pa                     0   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time                Summary  \\\n",
       "0                       1      5  1303862400  Good Quality Dog Food   \n",
       "1                       0      1  1346976000      Not as Advertised   \n",
       "\n",
       "                                                Text  \n",
       "0  I have bought several of the Vitality canned d...  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove symbols and numbers\n",
    "import re\n",
    "def cleaned(x):\n",
    "    return re.sub(r'[^a-zA-Z ]',' ',x)\n",
    "df['text_cleaned'] = df['Text'].apply(cleaned)\n",
    "\n",
    "# lower case transform\n",
    "df['text_cleaned'] = df['text_cleaned'].str.lower()\n",
    "\n",
    "# stem and filter out stop words\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def wordfilter(string, filtwords):\n",
    "    filtered = []\n",
    "    tokens = word_tokenize(string) \n",
    "    for word in tokens:\n",
    "        if word not in filtwords:\n",
    "            filtered.append(stemmer.stem(word))\n",
    "    return filtered\n",
    "\n",
    "for item, row in df.iterrows():\n",
    "    df.at[item, 'text_cleaned'] = wordfilter(row['text_cleaned'], stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product', 'arriv', 'label', 'jumbo', 'salt', 'peanut', 'peanut', 'actual', 'small', 'size', 'unsalt', 'sure', 'error', 'vendor', 'intend', 'repres', 'product', 'jumbo']\n"
     ]
    }
   ],
   "source": [
    "df[['Text','text_cleaned']][:2]\n",
    "print(df['text_cleaned'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model with all default parameters\n",
    "model_base = Word2Vec(df['text_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.96061814, -2.5470355 ,  0.47732458, -3.2855825 ,  1.5646207 ,\n",
       "       -1.4697863 ,  1.405279  ,  0.45318598, -2.8434055 ,  0.00930381,\n",
       "        0.52161384,  1.8094522 , -0.01619773,  1.0487529 , -0.07392865,\n",
       "       -2.1169996 ,  2.1019816 ,  2.6989503 ,  0.8351442 , -1.3653778 ,\n",
       "       -2.0431027 , -0.61818767,  0.20307854,  2.1668055 , -2.1621969 ,\n",
       "        1.1635364 ,  0.9542173 ,  0.92139494, -4.118723  ,  0.63691616,\n",
       "       -2.399269  ,  0.43543836,  0.5743402 , -0.96405   ,  0.543317  ,\n",
       "       -1.1213058 ,  2.9285383 ,  0.3826324 ,  0.7615425 ,  1.6145648 ,\n",
       "       -1.5099298 ,  3.4179177 , -4.099808  ,  0.99413687,  1.383523  ,\n",
       "       -2.2991269 ,  0.48511776, -0.13238025,  0.7414131 , -2.4375052 ,\n",
       "        3.5757852 , -5.8970757 ,  0.5406961 , -3.5622094 ,  0.20784187,\n",
       "        2.5606496 ,  2.5368028 , -0.40096894,  2.2933888 , -1.3364791 ,\n",
       "       -0.7300951 , -0.73663986,  0.32252777,  0.58058625, -2.1335118 ,\n",
       "       -0.96290153,  1.0450556 , -2.0887127 ,  6.347577  ,  1.4676281 ,\n",
       "       -0.9922591 , -1.5251837 , -2.1877568 ,  1.2209108 , -1.0709122 ,\n",
       "       -1.5383404 ,  1.5912632 ,  2.0822806 , -2.8919606 ,  1.6814653 ,\n",
       "       -0.03178216, -0.41509455, -0.4948197 , -0.7211618 , -1.3816364 ,\n",
       "        1.6435943 , -1.2333945 , -0.07880394, -3.979678  , -3.2428255 ,\n",
       "       -0.18529986, -1.5009769 ,  0.59417874,  0.7418685 , -2.6063294 ,\n",
       "        1.5369993 , -1.5209758 , -2.6312735 , -2.8596246 , -3.0852969 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base['peanut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lumpfish', 0.7236908078193665),\n",
       " ('roe', 0.7017508745193481),\n",
       " ('sturgeon', 0.6961816549301147),\n",
       " ('malossol', 0.6563703417778015),\n",
       " ('capelin', 0.6562642455101013)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.most_similar('caviar')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional models with changed parameters\n",
    "\n",
    "# skip gram base model\n",
    "model_skip = Word2Vec(df['text_cleaned'], sg=1)\n",
    "\n",
    "# CBOW with a larger window and larger min_count\n",
    "model_large_cbow = Word2Vec(df['text_cleaned'], window=30, min_count = 30, sg=0)\n",
    "\n",
    "# skip gram with a larger window and larger min_count\n",
    "model_large_skip = Word2Vec(df['text_cleaned'], window=30, min_count = 30, sg=1)\n",
    "\n",
    "# CBOW with a smaller window and smaller min_count\n",
    "model_small_cbow = Word2Vec(df['text_cleaned'], window=2, min_count = 2, sg=0)\n",
    "\n",
    "# skip gram with a smaller window and smaller min_count\n",
    "model_small_skip = Word2Vec(df['text_cleaned'], window=2, min_count = 2, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_base</th>\n",
       "      <td>lumpfish</td>\n",
       "      <td>roe</td>\n",
       "      <td>sturgeon</td>\n",
       "      <td>malossol</td>\n",
       "      <td>capelin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_skip</th>\n",
       "      <td>sturgeon</td>\n",
       "      <td>lumpfish</td>\n",
       "      <td>roe</td>\n",
       "      <td>malossol</td>\n",
       "      <td>osetra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_large_cbow</th>\n",
       "      <td>roe</td>\n",
       "      <td>beluga</td>\n",
       "      <td>sockey</td>\n",
       "      <td>russian</td>\n",
       "      <td>oyster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_large_skip</th>\n",
       "      <td>roe</td>\n",
       "      <td>keta</td>\n",
       "      <td>gourmetfoodstor</td>\n",
       "      <td>russian</td>\n",
       "      <td>salmon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_small_cbow</th>\n",
       "      <td>roe</td>\n",
       "      <td>lumpfish</td>\n",
       "      <td>keta</td>\n",
       "      <td>sturgeon</td>\n",
       "      <td>sprat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_small_skip</th>\n",
       "      <td>lumpfish</td>\n",
       "      <td>sturgeon</td>\n",
       "      <td>roe</td>\n",
       "      <td>malossol</td>\n",
       "      <td>capelin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0         1                2         3        4\n",
       "model_base        lumpfish       roe         sturgeon  malossol  capelin\n",
       "model_skip        sturgeon  lumpfish              roe  malossol   osetra\n",
       "model_large_cbow       roe    beluga           sockey   russian   oyster\n",
       "model_large_skip       roe      keta  gourmetfoodstor   russian   salmon\n",
       "model_small_cbow       roe  lumpfish             keta  sturgeon    sprat\n",
       "model_small_skip  lumpfish  sturgeon              roe  malossol  capelin"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models = [model_base, model_skip, model_large_cbow, model_large_skip, model_small_cbow, model_small_skip]\n",
    "\n",
    "pd.DataFrame([[tup[0] for tup in model.most_similar('caviar')[:5]] for model in all_models], index = ['model_base', 'model_skip', 'model_large_cbow', 'model_large_skip', 'model_small_cbow', 'model_small_skip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a fine foods review dataset, the first word we use to judge the different models qualitatively is 'caviar'.  \n",
    "\n",
    "The base model performs relatively well with the default parameters and CBOW implementation. Lumpfish is a relatively cheaper and available type of caviar but 'sturgeon' and 'roe' are words that almost exactly describe what caviar is.  \n",
    "\n",
    "When all model implementations are compared, there doesn't seem to be a huge difference in the top similar words to 'caviar'.  \n",
    "\n",
    "When the window for words is increased and the minimum word count for words is increased, there are more words that aren't as immediately applicable that show up. For example, oyster and salmon are common words that would exceed the minimum word count but might not be immediately applicable to caviar.  \n",
    "\n",
    "Meanwhile, reducing the window and minimum word count yields hyperspecific words that relate very well to caviar like 'keta' and 'malossol'.  \n",
    "\n",
    "The effect of skip gram models is most apparent when the window size is large. The large skip gram model yields similar tokens like gourmetfoodstor and salmon which are not seen in the other implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_base</th>\n",
       "      <td>turkey</td>\n",
       "      <td>chicken</td>\n",
       "      <td>meat</td>\n",
       "      <td>poultri</td>\n",
       "      <td>jerkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_skip</th>\n",
       "      <td>chicken</td>\n",
       "      <td>jerki</td>\n",
       "      <td>turkey</td>\n",
       "      <td>nesco</td>\n",
       "      <td>meat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_large_cbow</th>\n",
       "      <td>stroganoff</td>\n",
       "      <td>chicken</td>\n",
       "      <td>bullion</td>\n",
       "      <td>poultri</td>\n",
       "      <td>ostrich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_large_skip</th>\n",
       "      <td>jerki</td>\n",
       "      <td>turkey</td>\n",
       "      <td>chicken</td>\n",
       "      <td>beefi</td>\n",
       "      <td>meat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_small_cbow</th>\n",
       "      <td>turkey</td>\n",
       "      <td>chicken</td>\n",
       "      <td>meat</td>\n",
       "      <td>pork</td>\n",
       "      <td>fowl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_small_skip</th>\n",
       "      <td>chicken</td>\n",
       "      <td>turkey</td>\n",
       "      <td>jerkey</td>\n",
       "      <td>jerki</td>\n",
       "      <td>pork</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0        1        2        3        4\n",
       "model_base            turkey  chicken     meat  poultri   jerkey\n",
       "model_skip           chicken    jerki   turkey    nesco     meat\n",
       "model_large_cbow  stroganoff  chicken  bullion  poultri  ostrich\n",
       "model_large_skip       jerki   turkey  chicken    beefi     meat\n",
       "model_small_cbow      turkey  chicken     meat     pork     fowl\n",
       "model_small_skip     chicken   turkey   jerkey    jerki     pork"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[tup[0] for tup in model.most_similar('beef')[:5]] for model in all_models], index = ['model_base', 'model_skip', 'model_large_cbow', 'model_large_skip', 'model_small_cbow', 'model_small_skip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_base</th>\n",
       "      <td>(turkey, 0.7575222849845886)</td>\n",
       "      <td>(chicken, 0.7324066162109375)</td>\n",
       "      <td>(meat, 0.7079607248306274)</td>\n",
       "      <td>(poultri, 0.6909288763999939)</td>\n",
       "      <td>(jerkey, 0.6680481433868408)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_skip</th>\n",
       "      <td>(chicken, 0.7951224446296692)</td>\n",
       "      <td>(jerki, 0.7668876647949219)</td>\n",
       "      <td>(turkey, 0.7523636817932129)</td>\n",
       "      <td>(nesco, 0.7369696497917175)</td>\n",
       "      <td>(meat, 0.7326550483703613)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_large_cbow</th>\n",
       "      <td>(stroganoff, 0.709730327129364)</td>\n",
       "      <td>(chicken, 0.6739753484725952)</td>\n",
       "      <td>(bullion, 0.660982608795166)</td>\n",
       "      <td>(poultri, 0.6266921162605286)</td>\n",
       "      <td>(ostrich, 0.621371865272522)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_large_skip</th>\n",
       "      <td>(jerki, 0.7749670743942261)</td>\n",
       "      <td>(turkey, 0.7477177381515503)</td>\n",
       "      <td>(chicken, 0.743026852607727)</td>\n",
       "      <td>(beefi, 0.7388867139816284)</td>\n",
       "      <td>(meat, 0.7284117937088013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_small_cbow</th>\n",
       "      <td>(turkey, 0.7531991600990295)</td>\n",
       "      <td>(chicken, 0.7388721108436584)</td>\n",
       "      <td>(meat, 0.6888665556907654)</td>\n",
       "      <td>(pork, 0.6174705624580383)</td>\n",
       "      <td>(fowl, 0.6131404638290405)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_small_skip</th>\n",
       "      <td>(chicken, 0.7986695766448975)</td>\n",
       "      <td>(turkey, 0.7917496562004089)</td>\n",
       "      <td>(jerkey, 0.7655914425849915)</td>\n",
       "      <td>(jerki, 0.7493690252304077)</td>\n",
       "      <td>(pork, 0.7479978799819946)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                0  \\\n",
       "model_base           (turkey, 0.7575222849845886)   \n",
       "model_skip          (chicken, 0.7951224446296692)   \n",
       "model_large_cbow  (stroganoff, 0.709730327129364)   \n",
       "model_large_skip      (jerki, 0.7749670743942261)   \n",
       "model_small_cbow     (turkey, 0.7531991600990295)   \n",
       "model_small_skip    (chicken, 0.7986695766448975)   \n",
       "\n",
       "                                              1                             2  \\\n",
       "model_base        (chicken, 0.7324066162109375)    (meat, 0.7079607248306274)   \n",
       "model_skip          (jerki, 0.7668876647949219)  (turkey, 0.7523636817932129)   \n",
       "model_large_cbow  (chicken, 0.6739753484725952)  (bullion, 0.660982608795166)   \n",
       "model_large_skip   (turkey, 0.7477177381515503)  (chicken, 0.743026852607727)   \n",
       "model_small_cbow  (chicken, 0.7388721108436584)    (meat, 0.6888665556907654)   \n",
       "model_small_skip   (turkey, 0.7917496562004089)  (jerkey, 0.7655914425849915)   \n",
       "\n",
       "                                              3                             4  \n",
       "model_base        (poultri, 0.6909288763999939)  (jerkey, 0.6680481433868408)  \n",
       "model_skip          (nesco, 0.7369696497917175)    (meat, 0.7326550483703613)  \n",
       "model_large_cbow  (poultri, 0.6266921162605286)  (ostrich, 0.621371865272522)  \n",
       "model_large_skip    (beefi, 0.7388867139816284)    (meat, 0.7284117937088013)  \n",
       "model_small_cbow     (pork, 0.6174705624580383)    (fowl, 0.6131404638290405)  \n",
       "model_small_skip    (jerki, 0.7493690252304077)    (pork, 0.7479978799819946)  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([model.most_similar('beef')[:5] for model in all_models], index = ['model_base', 'model_skip', 'model_large_cbow', 'model_large_skip', 'model_small_cbow', 'model_small_skip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was also interesting to note that cosine similarity for skip gram is generally slightly higher than that returned by CBOW with the same hyperparameters.  \n",
    "\n",
    "For a token that is much more common like 'beef', there isn't a noticeable difference between skipgram and CBOW implementations for the default hyperparameters and the reduced window size and reduced minimum count. However, it is interesting to note that for a more common word like 'beef', the most similar words to 'beef' for the CBOW model are rarer words like 'stroganoff' and 'ostrich'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Skip-gram vs. Elmo vs. Bert\n",
    "\n",
    "Recent advances in NLP has resulted in multiple new techniques by which similarities between words are calculated by representing words as vectors.\n",
    "\n",
    "A few of the most prominent methods include the skip gram, elmo, and bert methods. Each of these methods offers up their respective advantages detailed in the below sections.\n",
    "\n",
    "1. Contextual understanding\n",
    "    * For skip gram, there is no contextual understanding of the words. The word 'play' for example would have the same vector representation no matter what sentence it is used in and what words there are around it.\n",
    "    * For both Elmo and Bert, they include deep contextual understanding of the words with relation to the words around them and type of usage. Both Elmo and Bert have bi-directional representations which help them to create context for each use case of the word. \n",
    "    \n",
    "2. Computational efficiency\n",
    "    * Bigrams and phrases for the skip gram model are selectively chosen to not use too much storage and compute. The subsampling of frequent words not just helps with better representing under-represented words but also helps with efficiency as well. Furthermore, there are no dense matrix multiplications used, making training extremely efficient.\n",
    "    * For Elmo, there is a constant need to balance overall language model perplexity with model size and computational requirements while maintaining a purely character based input representation. This has led to a halving of all embedding and hidden dimensions from the single best model.\n",
    "    \n",
    "3. Explainability\n",
    "    * Skip gram wins out on explainability in terms of ease of visualization due to the nature of linear representations of words. This is best shown in the famous king - man = queen example. The linear structure makes analogical reasoning easy. It is also easy to plot out different word clusters.\n",
    "    * For Elmo, there are 3 layers of representations for each input token, and there is added complexity that comes at the expense of being able to explain a token with a combination of these layers. The higher levels capture context dependent aspects of word meaning and lwer levels captures aspects of syntax. It is certainly more granular and provides better results but could be harder to explain.\n",
    "    * Similarly for Bert, the masked language model is an additional predictive layer not used in other models. The ability to capture phrases and pairs of sentences in a single token sequence adds to model performance but could be hard for explainability at scale.\n",
    "    \n",
    "4. Out of vocabulary\n",
    "    * Skip gram can only represent words and tokens within the training vocabulary.\n",
    "    * Other models are character based and could help with out of vocabulary instances. Furthermore, selected phrases or sentence pairs can be represented as tokens.\n",
    "    \n",
    "5. Customizability and ease of use\n",
    "    * Skip gram is very easy to use out of the box and a representation can be trained easily. You can also be highly case specific for hyperparameters and tune the model as you wish for the use case.\n",
    "    * Elmo is pretrained on a large scale and can be easily incorporated into a wide range of existing neural NLP architectures. For domain specific use, the biLM can be fine tuned on domain sepcific data.\n",
    "    * There is a minimal different between pre trained architecture and final downstream architecture and the same pre trained model can be used by a variety of tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
