{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Using nltk, textblob, and spacy for tokenization, stemming, and pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import textblob\n",
    "import spacy\n",
    "import nltk\n",
    "import tarfile\n",
    "import time\n",
    "from spacy.tokens import Doc\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data obtained from https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/Downloads/amazon-fine-food-reviews/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full corpus of all reviews\n",
    "corpus = ''\n",
    "for row in df.itertuples():\n",
    "    corpus += row[10]\n",
    "    corpus += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = corpus[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21858\n",
      "0.20186400413513184\n",
      "['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good']\n"
     ]
    }
   ],
   "source": [
    "# tokenizing in nltk\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(sample)\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "print(len(tokens))\n",
    "print(elapsed_time)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18905\n",
      "0.5088388919830322\n",
      "['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good']\n"
     ]
    }
   ],
   "source": [
    "# tokenizing in textblob\n",
    "\n",
    "blob = textblob.TextBlob(sample)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "tokens = blob.words\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "print(len(tokens))\n",
    "print(elapsed_time)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21563\n",
      "0.5657451152801514\n",
      "I have bought several of the Vitality canned dog food products and have found them all to be of good\n"
     ]
    }
   ],
   "source": [
    "# tokenizing in spacy\n",
    "\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "tokens = tokenizer(sample)\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "print(len(tokens))\n",
    "print(elapsed_time)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3578910827636719\n",
      "['I', 'have', 'bought', 'sever', 'of', 'the', 'vital', 'can', 'dog', 'food', 'product', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good']\n"
     ]
    }
   ],
   "source": [
    "# stemming in nltk\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(sample)\n",
    "\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "stemmed = [ps.stem(word) for word in tokens]\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "\n",
    "print(elapsed_time)\n",
    "print(stemmed[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14100313186645508\n",
      "['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'product', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good']\n"
     ]
    }
   ],
   "source": [
    "# stemming in textblob\n",
    "\n",
    "tokens = blob.words\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "stemmed = [textblob.Word(word).lemmatize() for word in tokens]\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "\n",
    "print(elapsed_time)\n",
    "print(stemmed[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019247055053710938\n",
      "['I', 'have', 'buy', 'several', 'of', 'the', 'Vitality', 'can', 'dog', 'food', 'product', 'and', 'have', 'find', 'them', 'all', 'to', 'be', 'of', 'good']\n"
     ]
    }
   ],
   "source": [
    "# stemming in spacy\n",
    "\n",
    "tokens = tokenizer(sample)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "stemmed = [token.lemma_ for token in tokens]\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "\n",
    "print(elapsed_time)\n",
    "print(stemmed[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4419121742248535\n",
      "[[('I', 'PRP')], [('h', 'VB'), ('a', 'DT'), ('v', 'NN'), ('e', 'NN')], [('b', 'NN'), ('o', 'NN'), ('u', 'JJ'), ('g', 'NN'), ('h', 'NN'), ('t', 'NN')], [('s', 'NN'), ('e', 'NN'), ('v', 'NN'), ('e', 'NN'), ('r', 'VBZ'), ('a', 'DT'), ('l', 'NN')], [('o', 'NN'), ('f', 'NN')], [('t', 'NN'), ('h', 'NN'), ('e', 'NN')], [('V', 'NNP'), ('i', 'NN'), ('t', 'VBP'), ('a', 'DT'), ('l', 'NN'), ('i', 'NN'), ('t', 'VBP'), ('y', 'NN')], [('c', 'VB'), ('a', 'DT'), ('n', 'JJ'), ('n', 'NN'), ('e', 'NN'), ('d', 'NN')], [('d', 'NN'), ('o', 'NN'), ('g', 'NN')], [('f', 'JJ'), ('o', 'NN'), ('o', 'NN'), ('d', 'NN')], [('p', 'NN'), ('r', 'NN'), ('o', 'NN'), ('d', 'NN'), ('u', 'JJ'), ('c', 'NN'), ('t', 'NN'), ('s', 'NN')], [('a', 'DT'), ('n', 'JJ'), ('d', 'NN')], [('h', 'VB'), ('a', 'DT'), ('v', 'NN'), ('e', 'NN')], [('f', 'JJ'), ('o', 'NN'), ('u', 'JJ'), ('n', 'NN'), ('d', 'NN')], [('t', 'NN'), ('h', 'NN'), ('e', 'NN'), ('m', 'NN')], [('a', 'DT'), ('l', 'NN'), ('l', 'NN')], [('t', 'NN'), ('o', 'NN')], [('b', 'NN'), ('e', 'NN')], [('o', 'NN'), ('f', 'NN')], [('g', 'NN'), ('o', 'MD'), ('o', 'VB'), ('d', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "# pos tagging in nltk\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(sample)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pos_tags = [nltk.pos_tag(word) for word in tokens]\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "\n",
    "print(elapsed_time)\n",
    "print(pos_tags[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.295608520507812e-05\n",
      "[('I', 'PRP'), ('have', 'VBP'), ('bought', 'VBN'), ('several', 'JJ'), ('of', 'IN'), ('the', 'DT'), ('Vitality', 'NNP'), ('canned', 'VBD'), ('dog', 'RP'), ('food', 'NN'), ('products', 'NNS'), ('and', 'CC'), ('have', 'VBP'), ('found', 'VBN'), ('them', 'PRP'), ('all', 'DT'), ('to', 'TO'), ('be', 'VB'), ('of', 'IN'), ('good', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "# pos tagging in textblob\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pos_tags = blob.pos_tags\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "\n",
    "print(elapsed_time)\n",
    "print(pos_tags[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015252828598022461\n",
      "['PRP', 'VBP', 'VBN', 'JJ', 'IN', 'DT', 'NNP', 'VBN', 'NN', 'NN', 'NNS', 'CC', 'VBP', 'VBN', 'PRP', 'DT', 'TO', 'VB', 'IN', 'JJ']\n"
     ]
    }
   ],
   "source": [
    "# pos tagging in spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tokens = nlp(sample)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pos = [token.tag_ for token in tokens]\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "\n",
    "print(elapsed_time)\n",
    "print(pos[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using regex for finding dates and emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Match all emails in text and compile a set of all found email addresses.\n",
    "\n",
    "email_re = re.compile(r'[a-zA-Z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}')\n",
    "x = email_re.findall('dd@dd.com not_an email email.email.com t3hisisanemail@dac.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dd@dd.com', 't3hisisanemail@dac.com']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Find all dates in text (e.g. 04/12/2019, April 20th 2019, etc).\n",
    "\n",
    "dates_re = re.compile(r'((0?[1-9])|(1[1|2])|jan[a-z]*|feb[a-z]*|mar[a-z]*|apr[a-z]*|may[a-z]*|jun[a-z]*|jul[a-z]*|aug[a-z]*|sep[a-z]*|oct[a-z]*|nov[a-z]*|dec[a-z]*)[\\/\\.\\-](?:[0-2]?[1-9]|3[0-1])[\\/\\.\\-]((?:19|20)[0-9]{2})', re.I)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = '01/1/2019 January/2/2019 dec-12-2020 3/2/2013 13/23/2018 10/23/2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01/1/2019', 'January/2/2019', 'dec-12-2020', '3/2/2013', '3/23/2018']\n"
     ]
    }
   ],
   "source": [
    "print ([x.group(0) for x in dates_re.finditer(test_dates)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
